% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the lecture slides for
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2012-2024
%%%%
%%%% ParLoop-slides.tex : slides about OpenMP's fork-join model
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{numberedframe}{Loop parallelism}
  Much of parallelism in scientific computing is in loops:
  \begin{itemize}
  \item Vector updates and inner products
  \item Matrix-vector and matrix-matrix operations
  \item Finite Element meshes
  \item Multigrid
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Work distribution}
  \begin{itemize}
  \item Suppose loop iterations are independent:
  \item Distribute them over the threads:
  \item Use \indextermtt{omp_get_thread_num} to determine disjoint subsets.
  \end{itemize}
\begin{lstlisting}
#pragma omp parallel
{
  int threadnum = omp_get_thread_num(),
    numthreads = omp_get_num_threads();
  int low = N*threadnum/numthreads,
    high = N*(threadnum+1)/numthreads;
  for (int i=low; i<high; i++)
    // do something with i
}
\end{lstlisting}
Discuss.
\end{numberedframe}

\begin{numberedframe}{Workshare constructs}
  Here's the two-step parallelization in OpenMP:
  \begin{itemize}
  \item You use the \indextermtt{parallel} directive to create a team of
    threads;
  \item then you use a `workshare' construct to distribute the
    work over the team.
  \item For loops that is the \indextermtt{for} (or \indextermtt{do}) construct.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Workshare construct for loops}
C: directive followed by statement or block:
\begin{verbatim}
#pragma omp parallel
{
#pragma omp for
  for (i=0; i<N; i++)
    ... something with i ...
}
\end{verbatim}
Fortran: matching end directive
\begin{verbatim}
!$omp parallel
!$omp do
  do i=1,n
    ... something with i ...
  end do
!$omp end do
!$omp end parallel
\end{verbatim}
\end{numberedframe}

\begin{numberedframe}{Stuff inside a parallel region}
\begin{multicols}{2}  
\begin{verbatim}
#pragma omp parallel
{
  code1();
#pragma omp for
  for (i=1; i<=4*N; i++) {
    code2();
  }
  code3();
}
\end{verbatim}
\columnbreak
\includegraphics[scale=.07]{parallel-do}
\end{multicols}
\end{numberedframe}

\begin{numberedframe}{Loops are static}
  \begin{itemize}
  \item No \lstinline{break} or so.
  \item \lstinline{continue} or \lstinline{cycle} allowed.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Loops are static'}
  \begin{itemize}
  \item Fixed upper bound;
  \item Simple loop increment;
  \item $\Rightarrow$~OpenMP needs to be able to calculate
    the number of iterations in advance.
  \item No \lstinline{while} loops.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{When is a loop parallel?}
  It's up to you!

  This is parallelizable:
\begin{lstlisting}
  for (int i=low; i<hi; i++)
     x[i] = // expression
\end{lstlisting}
Is this?
\begin{lstlisting}
for (int i=low; i<hi; i++)
   x[ f(i) ] = // expression
\end{lstlisting}
Histograms / binning.
\end{numberedframe}

\begin{exerciseframe}
  \input{sl:omp:loop-over-2}
\end{exerciseframe}

\begin{exerciseframe}
  Run the following snippet
  \begin{enumerate}
  \item Sequentially
  \item On one thread
  \item With more than one thread.
  \end{enumerate}
  \cverbatimsnippet{ompsineloop}
\end{exerciseframe}

\Level 1 {More about loops}

\begin{numberedframe}{Loop schedules}
  \begin{itemize}
  \item Static scheduling of iterations. \\
    (default in practice though not formally)\\
    Very efficient. Good if all iterations take the same amount of
    time.\\ \texttt{schedule(static)}
  \item Other possibility: dynamic.\\
    Runtime overhead; better if iterations do not take the same amount
    of time.\\
    \texttt{schedule(dynamic)}
  \end{itemize}

  Four threads, 8 tasks of decreasing size\\
  dynamic schedule is better:
  
  \includegraphics[scale=.07]{scheduling}
\end{numberedframe}

\begin{numberedframe}{Chunk size}
\small
  With $N$ iterations and $t$ threads:
  \begin{itemize}
  \item Static: each thread gets $N/t$ iterations.\\
    explicit chunk size: \texttt{schedule(static,123)}
  \item Dynamic: each thread gets $1$ iteration at a time\\
    explicit chunk size: \texttt{schedule(dynamic,45)}\\
  \end{itemize}
  \includegraphics[scale=.075]{schedules}
\end{numberedframe}

\begin{numberedframe}{Guided schedule}
  Use decreasing chunk size (with optional minimum
  chunk): \texttt{schedule(guided,6)}

  More flexible than dynamic, less overhead than dynamic.    
\end{numberedframe}

\begin{numberedframe}{Collapse}
  \begin{itemize}
  \item Parallelize loop nest
  \item More iterations $\Rightarrow$ better performance
  \item Inner loop needs to be directly nested\\
    bounds simple function of outer bounds and var
  \end{itemize}
\cverbatimsnippet{ompnbodycollapse}
\end{numberedframe}

\begin{numberedframe}{More loop topics}
  \begin{itemize}
  \item Ordered iterations: normally OpenMP can execute iterations in
    any sequence. You can force ordering if you absolutely have
    to. Bad for performance!
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Barriers}
  \begin{itemize}
  \item Barrier: synchronize all threads
  \item explicit: \lstinline{omp barrier}
  \item implicit: end of a workshare construct such as \lstinline{for}
  \item remove implicit barriers: \lstinline{nowait} clause
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Nowait}
\begin{lstlisting}
#pragma omp parallel nowait
for ( int i=0; i<N; ++i )
  x[i] = // something
#pragma omp parallel 
for ( int i=0; i<N; ++i )
  y[i] = ... x[i] ...
\end{lstlisting}
Needed:
  \begin{itemize}
  \item Same number of iterations
  \item Same schedule
  \end{itemize}
\end{numberedframe}

\Level 1 {Loop data}

\begin{numberedframe}{Where is the bug?}
\begin{lstlisting}
  int i,j;
#pragma omp parallel for private(temp)
   for(i=0;i<N;i++){
    for (j=0;j<M;j++){
       temp = b[i]*c[j];
       a[i][j] = temp * temp + d[i];
} }  
\end{lstlisting}
\end{numberedframe}

\Level 1 {Reduction}

\begin{numberedframe}{Reductions}
  \begin{itemize}
  \item Inner product loop:
\begin{verbatim}
s = 0.;
for (i=0; i<N; i++)
  s += x[i]*y[i];
\end{verbatim}
\item Use the \indextermtt{reduction(+:s)} clause.
  \item All the usual operations are available; you can also make your own.
  \end{itemize}
\end{numberedframe}

\begin{exerciseframe}[piadapt]
  \footnotesize
  \input ex:omp-pi-adapt
\end{exerciseframe}

\begin{numberedframe}{same exercise}
  \begin{enumerate}
  \item Use the \indextermtt{omp parallel for} construct to parallelize the loop.
    As in the previous lab, you may at first see an incorrect result.
    Use the \indextermtt{reduction} clause to fix this.
  \item Your code should now see a decent speedup, using up to 8~cores.
    However, it is possible to get completely linear speedup. For this
    you need to adjust the schedule.

    Start by using \indextermtt{schedule(static,$n$)}. Experiment with values
    for~$n$.  When can you get a better speedup? Explain this.
  \item Since this code is somewhat dynamic, try \indextermtt{schedule(dynamic)}.
    This will actually give a fairly bad result. Why?  Use
    \indextermtt{schedule(dynamic,$n$)} instead, and experiment with values
    for~$n$.
  \item Finally, use \indextermtt{schedule(guided)}, where OpenMP uses a
    heuristic.  What results does that give?
  \item \indextermtt{schedule(auto)} : leave it up to the system.
  \item \indextermtt{schedule(runtime)} : leave it up to environment variables;
    good for experimenting.
  \end{enumerate}
\end{numberedframe}

\begin{numberedframe}{Reduction on arrays, static}
  \cverbatimsnippet{creductstatic}
\end{numberedframe}

\begin{numberedframe}{Reduction on arrays, dynamic}
  \cverbatimsnippet{creductdynamic}
\end{numberedframe}

\begin{numberedframe}{Reduction on arrays, warning}
  Each thread gets a copy of the array on the stack\\
  $\Rightarrow$~possible stack overflow\\
  set \indexompshow{OMP_STACKSIZE}\\
  also see \indextermtt{ulimit} on the Unix level.
\end{numberedframe}

\begin{exerciseframe}[pi]
  \footnotesize
  \input ex:omp-pi
\end{exerciseframe}

%% \Level 1 {Affinity}

%% \begin{numberedframe}{Threads vs cores}
%%   \begin{itemize}
%%   \item Threads are software: instructions and local data
%%   \item OS can `migrate' threads
%%   \item Expensive because of data movement
%%   \item Destroys cache-locality.
%%   \item $\Rightarrow$~prevent migration
%%   \end{itemize}
%% \begin{lstlisting}
%% OMP_PROC_BIND=true
%% \end{lstlisting}
%% \lstinline{close/spread}, also \lstinline{OMP_PLACES=cores/sockets}
%% \end{numberedframe}

\Level 1 {Exercises}

\begin{exerciseframe}[jacobi]
  \input{ex:omp-jacobi1}
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:omp-jacobi2}
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:omp-jacobi3}
\end{exerciseframe}

\begin{exerciseframe}
  \input{ex:omp-jacobi4}
\end{exerciseframe}

\endinput

\begin{numberedframe}{}
\begin{lstlisting}
\end{lstlisting}
  \begin{itemize}
  \item 
  \end{itemize}
\end{numberedframe}

